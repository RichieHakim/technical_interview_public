{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing tests for python packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intro on testing suites: \\\n",
    "Neuroscientists aren't good at writing code. Generally, their goals are to collect data, apply some simple analyses to that data, then present the conclusions of that analysis. With time, a neuroscientist's personal bespoke analysis workflow might become crystallized into a pipeline with particular inputs and outputs. This is where the neuroscientist's expertise ends and they need help. Say they want to share their analysis method with others in their lab or in other labs. Now, the data that will be going into the code will derive from different sources, and the outputs must remain accurate. In addition, the code will be used on different operating systems, use different versions of underlying libraries (e.g. different numpy releases), and be subject to various maintenance iterations like bug fixes and new features. How can we ensure that the code always works or at least fails gracefully no matter the inputs, environment, or changes that occur. Software engineers have solved this problem of 'continuous integration': just write tests. Neuroscientists have generally never heard of a testing suite; their code just works on their system with their data. The goal of this coding exercise is to help an imaginary neuroscientist. You will consider the various usecases of a piece of code, understand it's functions and sharp edges, and then implement a simple testing suite that will allow for continugous integration of the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the various levels of complexity that can be applied in solving this problem, we provide 3 levels that you can choose from. The higher levels require using specific frameworks like pytest, setuptools, and pip, which are not prerequisites and can be learned on the job. If you are someone with experience using these tools, please consider skipping levels 1 and 2, and going straight to 3.\n",
    "\n",
    "### Level 1:\n",
    "- Look at the code below in the cell labeled `## codeblock 2`. We will treat this code as if it were written by a neuroscientist who asked you for help in preparing this code to share with others. Read the docstring and look over the code briefly before reading on. Your task is to do the following:\n",
    "    - **Improve the code**. Consider all the ways that it can fail and do the following:\n",
    "        - **check inputs**: Modify the function so that it checks it's input variables to ensure that no errors will occur and that the formatting makes sense.\n",
    "        - **improve numerical handling**: Improve the code to make it more robust to numerical errors (what if the input contains all NaNs, all zeros, has unexpected values, etc.). Take the liberty of assuming how you think each case should be handled; comment your code noting your assumptions.\n",
    "    - **Write test functions**. Each function should either throw a descriptive error for how the test failed (if it fails), else it should do nothing (if it passes). Follow the style guidelines for writing tests in the `pytest` package (https://docs.pytest.org/en/7.1.x/getting-started.html) and see a good example of a repo that implements tests here: (https://github.com/RichieHakim/ROICaT). Do the following:\n",
    "        - **ill-conditioned inputs**: Write a function that tests that the `pipeline` function can detect and throw an error upon receiving inputs that are ill-conditioned to run through the function. Again, feel free to make assumptions about what constitutes 'ill-conditioned' input data. One example might be if the `'trial_on'` variable were missing from the CSV file (see the docstring in the function).\n",
    "        - **accuracy**: Write some code that can generate fake data that will result in known outputs. Then write a test function that generates some fake data with known properties, passes that data through the `pipeline` function, and then checks that the output is accurate.\n",
    "\n",
    "Other info:\n",
    "- For level 1, feel free to simply write these functions and demonstrate their utility within this notebook.\n",
    "- In `## codeblock 1` you'll find some functions that are used to generate the input files to `pipeline`. These are meant to simulate data deriving from the neuroscientist's experiments. That said, it might be useful to draw some inspiration from the `make_fake_voltage_trace` function in deriving a function that can generate data with known properties for accuracy testing.\n",
    "\n",
    "### Level 2:\n",
    "\n",
    "Implement the level 1 code, and deploy it within a github repo. The repo should take on the following organization:\n",
    "- `my_pipeline`\n",
    "    - `__init__.py`\n",
    "    - `pipeline.py`\n",
    "- `tests`\n",
    "    - `test_all.py`\n",
    "- `conftest.py`\n",
    "- `setup.py`\n",
    "- `setup.cfg`\n",
    "- `README.md`\n",
    "- `MANIFEST.in`\n",
    "- `requirements.txt`\n",
    "\n",
    "- Rely on the structure found in this repo to guide you: https://github.com/RichieHakim/ROICaT\n",
    "- Please either make your repo public or if it is private, just add all of our usernames as collaborators: richiehakim, celiaberon, jbwallace123, bernardosabatini\n",
    "- The repo should be pip installable via `pip install .`\n",
    "- Calling pytest from CLI should work via `pytest`\n",
    "- If you get stuck in this exercise, reach out to us and we might be able to help.\n",
    "\n",
    "### Level 3:\n",
    "\n",
    "This one is a bit different (and was written by a different person). None of the below code is needed.\n",
    "The task is to write a testing suite for an existing package: https://github.com/jbwallace123/sabatini-glm-workflow. Fork the repo, determine it's function, and implement a set of tests that can be called by pytest from CLI.\n",
    "\n",
    "The repo is a simple analysis pipeline made by neuroscientist in our lab. Essentially it implements a generalized linear model on data collected from one of our tasks. The input data are expected to be timeseries with some particular columns. See the code in this folder to infer what these data types and shapes are: https://github.com/jbwallace123/sabatini-glm-workflow/tree/main/sglm.\n",
    "Data should be in a csv format with at least a SessionName, TrialNumber, Timestamp column. The predictors need to be binary and the response needs to be continuous variable.\n",
    "\n",
    "The tests should do the following: \n",
    "1. Make sure that the input data is the correct type.\n",
    "2. Make sure that the outputs are correct and do not vary wildly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "## codeblock 1\n",
    "\n",
    "def make_fake_parameters(filepath_save=r'~/Desktop/parameters.json'):\n",
    "    \"\"\"\n",
    "    Make a fake parameters file.\n",
    "\n",
    "    Args:\n",
    "        filepath_save (str):\n",
    "            Filepath to save the parameters file.\n",
    "    \"\"\"\n",
    "    import json\n",
    "    import numpy as np\n",
    "\n",
    "    parameters = {\n",
    "        'sample_rate': float(1000),\n",
    "        'threshold': float(np.random.randint(0, 100))\n",
    "    }\n",
    "\n",
    "    with open(filepath_save, 'w') as f:\n",
    "        json.dump(parameters, f)\n",
    "\n",
    "def make_fake_data_file(filepath_save=r'~/Desktop/data.csv'):\n",
    "    \"\"\"\n",
    "    Make a fake data file.\n",
    "\n",
    "    Args:\n",
    "        filepath_save (str):\n",
    "            Filepath to save the data file.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    N = 100000\n",
    "\n",
    "    data = {}\n",
    "    \n",
    "    data['trial_on'] = ((np.arange(N) % 1000) > 500).astype(np.bool_)\n",
    "    data['reward_on'] = ((np.arange(N) % 1000) > 750).astype(np.bool_)\n",
    "    data['light_on'] = ((np.arange(N) % 2000) > 900).astype(np.bool_)\n",
    "\n",
    "    voltages = {f\"neuron_{ii + 1}\": make_fake_voltage_trace(N) for ii in range(7)}\n",
    "\n",
    "    ## Make dataframe with named columns\n",
    "    data.update(voltages)\n",
    "    data_df = pd.DataFrame(data)\n",
    "\n",
    "    ## Save to CSV\n",
    "    data_df.to_csv(filepath_save, index=False)\n",
    "\n",
    "def make_fake_voltage_trace(n):\n",
    "    \"\"\"\n",
    "    Poisson spike train with some noise.\n",
    "    Baseline voltage: -70 mV\n",
    "    Spike voltage: +80 mV\n",
    "    Spike width: 2 ms\n",
    "    Noise: Gaussian with mean 0 and std 5 mV\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import scipy.signal\n",
    "\n",
    "    baseline_voltage = -70\n",
    "    spike_voltage = 100\n",
    "    threshold_voltage = -45\n",
    "    refractory_period = 40 # ms\n",
    "    spike_width = 1 # ms (std)\n",
    "    noise_mean = 0\n",
    "    variance = 8\n",
    "    noise_std = 10\n",
    "\n",
    "    sample_rate = 10000 # Hz\n",
    "\n",
    "    ## make a randomly varying trace\n",
    "    voltage_trace = np.random.normal(0, variance, n) + baseline_voltage\n",
    "    spike_bool = voltage_trace > threshold_voltage\n",
    "    ## remove spikes within the refractory period\n",
    "    spike_times = np.where(spike_bool)[0].astype(np.float32)\n",
    "    spike_times[np.where(np.diff(spike_times, prepend=0) < refractory_period)[0]] = np.nan\n",
    "    spike_times = spike_times[~np.isnan(spike_times)]\n",
    "    ## make a new boolean trace with the refractory spikes removed\n",
    "    spike_bool = np.isin(np.arange(n), spike_times)\n",
    "    ## convolve boolean with gaussian to approximate the timecourse of a spike\n",
    "    spike_kernel = scipy.signal.windows.gaussian(spike_width/1000 * sample_rate * 5, spike_width/1000 * sample_rate) * spike_voltage\n",
    "    spike_kernel = spike_kernel / np.max(spike_kernel)\n",
    "    voltage_trace += (np.convolve(spike_bool, spike_kernel, mode='same') * spike_voltage) + np.random.normal(noise_mean, noise_std, n)\n",
    "\n",
    "    return voltage_trace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_fake_data_file('/Users/richardhakim/Desktop/data.csv')\n",
    "\n",
    "make_fake_parameters('/Users/richardhakim/Desktop/params.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## codeblock 2\n",
    "\n",
    "def pipeline(filepath_data, filepath_parameters):\n",
    "    \"\"\"\n",
    "    Fake neuroscience analysis pipeline.\n",
    "    This code counts the number of spikes fired by neurons.\n",
    "    The input data are timeseries traces of the voltage of individual neurons.\n",
    "    The code applies some signal processing techniques to clean the data, \n",
    "     then extracts and counts the number of peaks, which are assumed to be spikes.\n",
    "    The output is a dataframe with the total number of spikes summed across all neurons\n",
    "     during the trial periods for different experimental conditions.\n",
    "\n",
    "    Required libraries:\n",
    "        - pandas\n",
    "        - numpy\n",
    "        - scipy\n",
    "        - matplotlib\n",
    "\n",
    "    Args:\n",
    "        filepath_data (str):\n",
    "            Filepath to CSV file.\n",
    "            CSV file should have one row of headers.\n",
    "            Each column should correspond to a timeseries trace and the first row should be the name of that trace.\n",
    "            The data should contain the 3 critical columns that correspond to temporal epochs within the experiment: \n",
    "                - 'trial_on'\n",
    "                - 'reward_on'\n",
    "                - 'light_on'\n",
    "            All other columns are expected to be the voltage traces of individual neurons.\n",
    "        filepath_parameters (str):\n",
    "            Filepath to JSON file that can be read into python as a python dictionary object.\n",
    "            The parameters are expected to contain the following elements with defined 'key' names:\n",
    "                - 'sample_rate' (float): frequency at which data samples were collected.\n",
    "                - 'threshold' (float): voltage above which the voltage must reach to be considered a valid spike.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    import numpy as np\n",
    "\n",
    "    data = pd.read_csv(filepath_data)\n",
    "    with open(filepath_parameters, 'r') as f:\n",
    "        parameters = json.load(f)\n",
    "\n",
    "    keys_trial = ['trial_on', 'reward_on', 'light_on']\n",
    "    t, r, l = (data[key] for key in keys_trial)\n",
    "    keys_neurons = [key for key in data.keys() if key not in keys_trial]\n",
    "    st = [count_spikes(data[key]) for key in keys_neurons] # 'spike_times'\n",
    "    st_cat = np.concatenate(st)\n",
    "\n",
    "    bool_to_idx = lambda x: np.where(x)[0]\n",
    "    idx_conditions = {\n",
    "        't': bool_to_idx(t), ## trial_on\n",
    "        'r': bool_to_idx(r), ## reward_on\n",
    "        'l': bool_to_idx(l), ## light_on\n",
    "        'tr': bool_to_idx(t * r),\n",
    "        'tl': bool_to_idx(t * l),\n",
    "        'rl': bool_to_idx(r * l),\n",
    "        'trl': bool_to_idx(t * r * l),\n",
    "    }    \n",
    "\n",
    "    ns_conditions = {key: np.isin(st_cat, val).sum() for key, val in idx_conditions.items()}\n",
    "    return ns_conditions\n",
    "    \n",
    "\n",
    "def count_spikes(trace, sample_rate=10000, threshold=10):\n",
    "    ## smooth the trace\n",
    "    w = sample_rate / 500 # roughly the n_samples of a spike\n",
    "    w = int(w + np.remainder(w, 2)) # make odd\n",
    "    trace_smooth = scipy.signal.savgol_filter(\n",
    "        x=trace,\n",
    "        window_length=w, \n",
    "        polyorder=2,\n",
    "    )\n",
    "\n",
    "    ## find peaks (spike times)\n",
    "    peaks, _ = scipy.signal.find_peaks(\n",
    "        x=trace_smooth,\n",
    "        height=threshold,\n",
    "        distance=(2/1000 * sample_rate),\n",
    "    )\n",
    "    \n",
    "    return peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'t': 306, 'r': 147, 'l': 361, 'tr': 147, 'tl': 191, 'rl': 106, 'trl': 106}"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline(filepath_data='/Users/richardhakim/Desktop/data.csv', filepath_parameters='/Users/richardhakim/Desktop/params.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
